{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "    \n",
    "import h5py\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dill as pickle\n",
    "except ImportError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Precomputed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "# Loading Image Features\n",
    "train_features = load(open(\"/home/vinit/Desktop/Projects/Image Captioning/Dataset/encoded_train_images.pkl\", \"rb\"))\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "# Load Train Descriptions, which contains, 5 captions corresponding to a key\n",
    "train_descriptions = load(open(\"/home/vinit/Desktop/Projects/Image Captioning/Dataset/train_descriptions.pkl\", \"rb\"))\n",
    "print('Photos: train=%d' % len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix shape = (2057, 200)\n"
     ]
    }
   ],
   "source": [
    "# Loading embedding_matrix\n",
    "embedding_matrix = load(open(\"/home/vinit/Desktop/Projects/Image Captioning/Dataset/Embedding_Matrix.pkl\", \"rb\"))\n",
    "print('Embedding Matrix shape = (%d, %d)' % (embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordtoix Length = 2056\n"
     ]
    }
   ],
   "source": [
    "# Loading wordtoix\n",
    "wordtoix = load(open(\"/home/vinit/Desktop/Projects/Image Captioning/Dataset/wordtoix.pkl\", \"rb\"))\n",
    "print('Wordtoix Length = %d' % len(wordtoix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 34 # computer from file 2\n",
    "epochs = 10\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    \n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            \n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0\n",
    "    return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 35 # computer from file 2\n",
    "epochs = 10\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features['2513260012_03d33305cf.jpg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 2057, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 35 \n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)//number_pics_per_bath\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "max_length,vocab_size ,embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "my_model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 35, 200)      411400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 35, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2057)         528649      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,998,353\n",
      "Trainable params: 1,998,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7f605fa365c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.layers[2].set_weights([embedding_matrix])\n",
    "my_model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.save('./model_weights/model_' + 'test' + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "number_pics_per_bath = 6\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 4.3032\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 606s 606ms/step - loss: 3.5316\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 3.2799\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 609s 609ms/step - loss: 3.1282\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 608s 608ms/step - loss: 3.0155\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 648s 648ms/step - loss: 2.9290\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 674s 674ms/step - loss: 2.8612\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 667s 667ms/step - loss: 2.7982\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 736s 736ms/step - loss: 2.7479\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 758s 758ms/step - loss: 2.7057\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 662s 662ms/step - loss: 2.6661\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 660s 660ms/step - loss: 2.6324\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 664s 664ms/step - loss: 2.6025\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 705s 705ms/step - loss: 2.5753\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 705s 705ms/step - loss: 2.5499\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 705s 705ms/step - loss: 2.5308\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 706s 706ms/step - loss: 2.5070\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 968s 968ms/step - loss: 2.4908\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 774s 774ms/step - loss: 2.4751\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 643s 643ms/step - loss: 2.4581\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 2.4415\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 640s 640ms/step - loss: 2.4261\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 645s 645ms/step - loss: 2.4149\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 2.4056\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 602s 602ms/step - loss: 2.3927\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 2.3816\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 678s 678ms/step - loss: 2.3728\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 635s 635ms/step - loss: 2.3629\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 634s 634ms/step - loss: 2.3552\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 603s 603ms/step - loss: 2.3454\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 602s 602ms/step - loss: 2.3394\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 602s 602ms/step - loss: 2.3310\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 602s 602ms/step - loss: 2.3247\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 601s 601ms/step - loss: 2.3173\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 615s 615ms/step - loss: 2.3109\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 667s 667ms/step - loss: 2.3055\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 2.2965\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 609s 609ms/step - loss: 2.2937\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 608s 608ms/step - loss: 2.2860\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 609s 609ms/step - loss: 2.2789\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
    "    my_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    my_model.save('./model_weights/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/vinit/Desktop/Projects/Image Captioning/Caption_generator_Model.pkl', 'wb') as model:\n",
    "    pickle.dump(my_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
